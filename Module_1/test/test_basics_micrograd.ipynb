{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# micrograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Backpropagation Using Micrograd\n",
    "\n",
    "In this section, I have explained the basic logic of backpropagation using **Micrograd** with simple scalar values. The code below demonstrates how changes in inputs `a` and `b` affect the final value of `g` through a sequence of operations. \n",
    "\n",
    "### Chain Rule & Backpropagation\n",
    "Backpropagation is based on the **chain rule**, which helps compute the gradient of `g` with respect to `a` and `b`. The chain rule states that if a variable depends on intermediate values, their derivatives must be multiplied along the computational path.\n",
    "\n",
    "1. We first perform a **forward pass**, computing `g` step by step.\n",
    "2. During the **backward pass**, gradients are propagated backward using the chain rule:\n",
    "   - Each node's gradient is computed as the product of its own derivative and the gradient of its dependent nodes.\n",
    "   - This continues recursively until reaching the inputs `a` and `b`.\n",
    "   - The computed gradients (`dg/da` and `dg/db`) indicate how small changes in `a` and `b` influence `g`.\n",
    "\n",
    "### Interpretation of Results\n",
    "- The output `g.data = 24.7041` is the final computed value.\n",
    "- `a.grad = 138.8338` and `b.grad = 645.5773` represent the sensitivity of `g` to `a` and `b`.\n",
    "- These gradients can be used to **optimize** `g` towards a desired outcome.\n",
    "\n",
    "### Tensors in Neural Networks\n",
    "In real neural networks, we work with large arrays of values. Instead of scalars, we use **tensors**, enabling **parallel computation** for efficiency. The fundamental logic of backpropagation remains the same, but tensors provide computational speed and scalability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting micrograd\n",
      "  Downloading micrograd-0.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Downloading micrograd-0.1.0-py3-none-any.whl (4.9 kB)\n",
      "Installing collected packages: micrograd\n",
      "Successfully installed micrograd-0.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install micrograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.7041\n",
      "138.8338\n",
      "645.5773\n"
     ]
    }
   ],
   "source": [
    "from micrograd.engine import Value\n",
    "\n",
    "a = Value(-4.0)\n",
    "b = Value(2.0)\n",
    "c = a + b\n",
    "d = a * b + b**3\n",
    "c += c + 1\n",
    "c += 1 + c + (-a)\n",
    "d += d * 2 + (b + a).relu()\n",
    "d += 3 * d + (b - a).relu()\n",
    "e = c - d\n",
    "f = e**2\n",
    "g = f / 2.0\n",
    "g += 10.0 / f\n",
    "print(f'{g.data:.4f}') # prints 24.7041, the outcome of this forward pass\n",
    "g.backward()\n",
    "print(f'{a.grad:.4f}') # prints 138.8338, i.e. the numerical value of dg/da\n",
    "print(f'{b.grad:.4f}') # prints 645.5773, i.e. the numerical value of dg/db"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
